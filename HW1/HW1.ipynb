{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1. [25 points] Calculus of Variations in Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability density function (PDF) of an $n$-dimensional continuous random vector $X$ is a deterministic function $u(x)$, where $x\\in\\Omega\\subseteq\\mathbb{R}^{n}$, such that $u(x)\\geq 0$, and $\\int_{\\Omega}u(x){\\mathrm{d}}x = 1$. The expected value of any nonlinear function $\\phi(x)$, denoted here as $\\mathbb{E}\\left[\\phi(x)\\right]$, is defined as the integral\n",
    "$$\\mathbb{E}\\left[\\phi(x)\\right] := \\int_{\\Omega}\\phi(x)u(x){\\mathrm{d}}x.$$\n",
    "A well-known example of the expectation integral of above type is the \"entropy\" $\\mathbb{E}\\left[-\\log u(x)\\right] = -\\int_{\\Omega}u(x)\\log u(x){\\mathrm{d}}x$, i.e., $\\phi(\\cdot)\\equiv -\\log u(\\cdot)$. The entropy functional is known to be strictly concave function of $u$, i.e., the problem of maximizing entropy, or equivalently, minimizing $\\mathbb{E}\\left[\\log u(x)\\right]$ is a convex optimization problem. In this exercise, we will assume that $\\Omega$ is compact, and denote its (Lebesgue) volume as ${\\mathrm{vol}}(\\Omega) := \\int_{\\Omega}{\\mathrm{d}}x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) [6 + 4 + 5 = 15 points] Maximizing entropy vs. minimizing expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a non-negative function $f(x)$, and a scalar $\\beta>0$, we want to determine the PDF $u(x)$ that minimizes\n",
    "$$I(u) = \\mathbb{E}\\left[\\log u + \\beta\\, f\\right] = \\int_{\\Omega}\\big\\{u(x)\\log u(x) + \\beta\\, f(x)u(x)\\big\\}{\\mathrm{d}}x.$$\n",
    "By changing the value of $\\beta$, we can trade-off the relative importance of maximizing entropy and minimizing the expected value of $f$. In optimization literature, $\\beta$ is called a \"regularizing parameter\". \n",
    "\n",
    "(a.1) Using Euler-Lagrange equation, determine the PDF $u^{*}$ that minimizes $I(u)$. \n",
    "\n",
    "(a.2) Carefully argue why $u^{*}$ is a minimizer (not maximizer or saddle), and why it is the unique minimizer.\n",
    "\n",
    "(a.3) By specializing your answer in (a.1), determine $u^{*}$ for $\\beta=0$, and interpret your result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) [6 + 4 = 10 points] Expectation constrained entropy maximization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose now we would like to minimize $I(u) = \\mathbb{E}\\left[\\log u\\right]$ subject to the constraints $\\mathbb{E}\\left[g_{i}(x)\\right] = a_{i}$, $i=1,...,m$, where the vectors $g(x) = (g_{1}(x), ..., g_{m}(x))^{\\top}$ and $a = (a_1, ..., a_m)^{\\top}$ are given.\n",
    "\n",
    "(b.1) Prove that the minimizing PDF $u^{*}$ must be of exponential family, i.e., \n",
    "\n",
    "$$u^{*}(x) = \\displaystyle\\frac{\\exp\\left(-\\langle\\lambda,g(x)\\rangle\\right)}{\\int_{\\Omega}\\exp\\left(-\\langle\\lambda,g(x)\\rangle\\right){\\mathrm{d}}x}$$\n",
    "\n",
    "for some $m\\times 1$ vector $\\lambda$ that is independent of $x$.\n",
    "\n",
    "(b.2) Carefully argue why $u^{*}$ in (b.1) is a minimizer, and why it is the unique minimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2. Total Variation Denoising "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a grayscale image $u(x)$ where $x\\in\\Omega\\subset\\mathbb{R}^{2}$. We can think of the value of $u(x)$ as the brightness of the pixel at location $x$. Given a noise corrupted image $f(x)$ in $\\Omega$, consider the problem of \"denoising\", i.e., recovering the original image. A popular formulation for this problem is to minimize\n",
    "\n",
    "$$I(u) = \\displaystyle\\int_{\\Omega}\\bigg\\{\\frac{1}{2}\\left(u-f\\right)^{2} + \\beta\\parallel\\nabla u\\parallel_{2}\\bigg\\}{\\mathrm{d}}x,$$\n",
    "\n",
    "where $\\beta>0$ is a regularizing parameter. The term $\\frac{1}{2}(u-f)^{2}$ is called the \"data fidelity term\" and ensures the restored image to be close to the input image $f$. The term $\\int_{\\Omega}\\parallel\\nabla u\\parallel_{2}{\\mathrm{d}}x$ is called the \"total variation\" of $u$, minimizing which promotes noise removal. Notice that the norm in the total variation term is not squared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) (8 + 2 = 10 points) Since the Lagrangian associated with the above calculus of variations problem is not differentiable at $\\nabla u = 0$, we consider minimizing\n",
    "\n",
    "$$I_{\\varepsilon}(u) = \\displaystyle\\int_{\\Omega}\\bigg\\{\\frac{1}{2}\\left(u-f\\right)^{2} + \\beta\\sqrt{\\parallel\\nabla u\\parallel_{2}^{2} + \\varepsilon^{2}}\\bigg\\}{\\mathrm{d}}x$$\n",
    "\n",
    "for small $\\varepsilon>0$. It is possible (but somewhat technical) to prove that in the limit $\\varepsilon\\downarrow 0$, the minimizer of $I_{\\varepsilon}(u)$ goes to that of $I(u)$. Derive the Euler-Lagrange equation associated with $I_{\\varepsilon}(u)$. \n",
    "\n",
    "Argue why it is natural to impose the Neumann boundary condition $\\frac{\\partial u}{\\partial n} = 0$ for all $x\\in\\partial\\Omega$. Here, $n$ denotes the outward normal to the boundary $\\partial\\Omega$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) (5 points) Derive the gradient descent PDE associated with the Euler-Lagrange equation in part (a)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "(c) (10 points) Take a grayscale image of your choice. Implement a numerical discrete time marching for the gradient descent PDE in part (b), with forward difference approximation for $\\frac{\\partial u}{\\partial t}$, central difference approximation  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
